version: '3.8'  # Upgrade to 3.8 for better features

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      # We don't need POSTGRES_DB because our init.sql creates multiple DBs
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 5s
      retries: 5

  mlflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.mlflow
    container_name: mlflow_server
    restart: always
    ports:
      - "5000:5000"
    environment:
      - BACKEND_STORE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:${DB_PORT}/mlflow_db
      - ARTIFACT_ROOT=/mlartifacts
    volumes:
      - mlflow_artifacts:/mlartifacts
    # --- FIX IS HERE: Force host to 0.0.0.0 ---
    command: >
      sh -c "mlflow server
      --backend-store-uri $${BACKEND_STORE_URI}
      --default-artifact-root /mlartifacts
      --host 0.0.0.0
      --port 5000"
    depends_on:
      postgres:
        condition: service_healthy

  airflow-init:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:${DB_PORT}/airflow_db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_ADMIN_USER}
      - _AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_ADMIN_PASS}
    command: bash -c "airflow db migrate && airflow users create --username $${_AIRFLOW_WWW_USER_USERNAME} --password $${_AIRFLOW_WWW_USER_PASSWORD} --firstname Peter --lastname Pan --role Admin --email ${AIRFLOW_ADMIN_EMAIL}"
    depends_on:
      postgres:
        condition: service_healthy
      fix-permissions:                # <--- ADD THIS
        condition: service_completed_successfully # <--- WAIT FOR IT TO FINISH

  airflow-run:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:${DB_PORT}/airflow_db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    # PRO TIP: Best Practice for Hybrid Development
    # Even though your Dockerfile 'COPY's the code, we KEEP these volumes.
    # Why?
    # 1. In Production (Kubernetes): The code is baked in (Immutable).
    # 2. In Local (Compose): These volumes OVERWRITE the baked code with your live local files.
    # This gives you "Hot Reload" locally, but "Stability" in Prod.
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - mlflow_artifacts:/mlartifacts # <--- Mount volume here (Must match MLflow path)
    ports:
      - "8081:8080"
    command: bash -c "airflow webserver & airflow scheduler"
    depends_on:
      airflow-init:
        condition: service_completed_successfully


  fix-permissions:
    image: alpine
    user: root
    command: sh -c "chown -R 50000:0 /mlartifacts && chmod -R 777 /mlartifacts"
    volumes:
      - mlflow_artifacts:/mlartifacts

  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    restart: always
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    ports:
      - "8000:8000"
    volumes:
      - mlflow_artifacts:/mlartifacts # <--- Mount volume here (Read access)
    depends_on:
      mlflow:
        condition: service_started

volumes:
  postgres_data:
  mlflow_artifacts:  # <--- NEW SHARED VOLUME